{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1EVA5efhEcO",
        "outputId": "34bffa22-0466-4b93-cb66-bf329adfa0d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain_core langchain_openai langchain_community langchain-qdrant qdrant-client langsmith openai tiktoken cohere lxml protobuf===3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain_experimental langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcOiIFIlrkab",
        "outputId": "a96b9805-aaf6-405e-ce19-c6917602c0a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m206.9/206.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU pymupdf ragas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnoPBdMVtvnP",
        "outputId": "7651cfef-1c6d-4234-b98b-ba524a25e6d5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m655.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m970.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m185.7/185.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m952.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m677.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swWAJJwGsY-q",
        "outputId": "6e88f91d-d4ed-4fbc-d0de-e6ef2dd52ca0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key:路路路路路路路路路路\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "RhZ2wWhZsZXq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "ai_framework_document = PyMuPDFLoader(file_path=\"https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf\").load() #22,204 words, 64 pages\n",
        "ai_blueprint_document = PyMuPDFLoader(file_path=\"https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf\").load() #31,393 words, 73 pages"
      ],
      "metadata": {
        "id": "K6gykzwhseeM"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **Default Chunking Strategy**\n",
        "\n",
        "`semantic chunking`\n",
        "\n",
        "####  **Experimental Chunking Strategy**\n",
        "\n",
        "`semantic chunking` (initial) then `fixed chunk size` + `overlap`\n",
        "\n",
        "####  **Rationale**\n",
        "\n",
        "I chose semantic chunking because meaning is important. By combining semantic chunking with fixed chunk size and overlaps helps ensure balanced and retain/retrieve contextual information better than using the default chunking strategy alone.\n",
        "\n",
        "####  **Additional notes**\n",
        "Using my Experimental Chunking Strategy, plus TE3-large for chunking and embedding generation, have produced the best quality of response so far. (See comparison with base RAG below)"
      ],
      "metadata": {
        "id": "TaR0zDMO5IpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Selecting TE3-large because the combined documents is 53,597 words long and is 137 pages\n",
        "text_splitter = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-large\"))\n",
        "base_text_splitter = SemanticChunker(OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "3X5vJUjQsgyO"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metadata_generator(document, name):\n",
        "  \"\"\"\n",
        "  Adds metadata name to the semantically chunked document\n",
        "  \"\"\"\n",
        "  collection = text_splitter.split_documents(document)\n",
        "  for doc in collection:\n",
        "    doc.metadata[\"source\"] = name\n",
        "  return collection"
      ],
      "metadata": {
        "id": "Oy23FDivwAnL"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The next chunking step after semantic chunking\n",
        "fixed_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "recursive_framework_document = fixed_text_splitter.split_documents(metadata_generator(ai_framework_document, \"AI Framework\"))\n",
        "recursive_blueprint_document = fixed_text_splitter.split_documents(metadata_generator(ai_blueprint_document, \"AI Blueprint\"))"
      ],
      "metadata": {
        "id": "AWmzHtRs0Tv-"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since both documents have metadata, we can now combine them before vectorization\n",
        "combined_documents = recursive_framework_document + recursive_blueprint_document\n",
        "base_combined_documents = metadata_generator(ai_framework_document, \"AI Framework\") + metadata_generator(ai_blueprint_document, \"AI Blueprint\")"
      ],
      "metadata": {
        "id": "nv5r13BOwc8v"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Selecting TE3-large because the combined documents is 53,597 words long and is 137 pages\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n"
      ],
      "metadata": {
        "id": "mqXzPkzStJC4"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents=combined_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"AI Policy\"\n",
        ")\n",
        "\n",
        "base_vectorstore = Qdrant.from_documents(\n",
        "    documents=base_combined_documents,\n",
        "    embedding=OpenAIEmbeddings(),\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Base AI Policy\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "ia8KlELKuTZe"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()\n",
        "base_retriever = base_vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "ympqzGuGucw9"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and question, you must answer the question based only on context.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ],
      "metadata": {
        "id": "PPLWhYmLujKU"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")"
      ],
      "metadata": {
        "id": "khX3pcAVulCJ"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | rag_prompt | llm | StrOutputParser()\n",
        ")\n",
        "\n",
        "base_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | rag_prompt | llm | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "EhbQ5Qetunib"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "output1 = rag_chain.invoke({\"question\" : \"What is the AI framework all about?\"})\n",
        "output2 = base_rag_chain.invoke({\"question\" : \"What is the AI framework all about?\"})\n",
        "\n",
        "html = f\"\"\"\n",
        "<table>\n",
        "    <tr>\n",
        "        <th style=\"text-align: left;\">Response from my strategy</th>\n",
        "        <th style=\"text-align: left;\">Response from Base RAG</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td style=\"text-align: left;\">{output1}</td>\n",
        "        <td style=\"text-align: left;\">{output2}</td>\n",
        "    </tr>\n",
        "</table>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(html))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "tdB34gz273sC",
        "outputId": "5599d1ac-019d-44ad-bf44-d4e8bbc5521b"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<table>\n",
              "    <tr>\n",
              "        <th style=\"text-align: left;\">Response from my strategy</th>\n",
              "        <th style=\"text-align: left;\">Response from Base RAG</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <td style=\"text-align: left;\">The AI framework is centered on the evaluation and development of AI products, services, and systems. It is being developed by the National Institute of Standards and Technology (NIST) through a consensus-driven, open, transparent, and collaborative process. The framework aims to foster innovative approaches to address various characteristics of trustworthiness, including accuracy, explainability, interpretability, reliability, privacy, robustness, safety, security (resilience), and the mitigation of unintended and/or harmful biases and uses. It also considers principles such as transparency, accountability, and fairness throughout the lifecycle of AI technologies and systems, from pre-design to deployment, use, and evaluation. The framework is expected to be released in the winter of 2022-23.</td>\n",
              "        <td style=\"text-align: left;\">The AI framework mentioned in the provided context is primarily concerned with managing the risks posed by artificial intelligence (AI) to individuals, organizations, and society. Specifically:\n",
              "\n",
              "1. **NIST AI Risk Management Framework**: Developed by the National Institute of Standards and Technology (NIST), this framework aims to incorporate trustworthiness considerations into the design, development, use, and evaluation of AI products, services, and systems. It focuses on characteristics such as accuracy, explainability, interpretability, reliability, privacy, robustness, safety, security, and mitigation of unintended and/or harmful bias and uses. The framework is being developed through a consensus-driven, open, transparent, and collaborative process.\n",
              "\n",
              "2. **Blueprint for an AI Bill of Rights**: This is a set of five principles and associated practices intended to guide the design, use, and deployment of automated systems to protect the rights of the American public. It was developed through extensive public consultation and aims to align AI systems with democratic values and protect civil rights, civil liberties, and privacy.\n",
              "\n",
              "3. **Executive Order 13960**: This order mandates that certain federal agencies adhere to nine principles when designing, developing, acquiring, or using AI, ensuring that AI is lawful, purposeful, accurate, safe, understandable, responsible, regularly monitored, transparent, and accountable.\n",
              "\n",
              "Overall, the AI framework seeks to promote the safe and effective use of AI technologies while ensuring they are trustworthy and aligned with ethical and democratic values.</td>\n",
              "    </tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}